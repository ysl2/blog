# AI workflow software

## Chatbots overview

| URL                                             | Description                                                                                                                       |
| ----------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
| <https://github.com/langgenius/dify>            | Production-ready platform for agentic workflow development.                                                                       |
| <https://github.com/ChatGPTNextWeb/NextChat>    | Light and Fast AI Assistant. Support: Web \| iOS \| MacOS \| Android \| Linux \| Windows                                          |
| <https://github.com/infiniflow/ragflow>         | RAGFlow is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding.                       |
| <https://github.com/Mintplex-Labs/anything-llm> | The all-in-one Desktop & Docker AI application with built-in RAG, AI agents, No-code agent builder, MCP compatibility, and more.  |
| <https://github.com/chatboxai/chatbox>          | User-friendly Desktop Client App for AI Models/LLMs (GPT, Claude, Gemini, Ollama...)                                              |
| <https://github.com/CherryHQ/cherry-studio>     | ğŸ’ Cherry Studio is a desktop client that supports for multiple LLM providers.                                                    |
| <https://github.com/getAsterisk/deepclaude>     | A high-performance LLM inference API and Chat UI that integrates DeepSeek R1's CoT reasoning traces with Anthropic Claude models. |

## Openai official ChatGPT

éœ€è¦ï¼š
iosè®¾å¤‡ï¼Œæ¢¯å­ç¯å¢ƒï¼Œç¾åŒºè´¦å·(åœ°åŒºåœ¨ç¾å›½äº”ä¸ªå…ç¨å·å†…)ï¼Œæ”¯ä»˜å®ï¼Œchatgptè´¦å·

æ­¥éª¤ï¼š

- iosè®¾å¤‡ï¼Œåœ°åŒºåˆ‡æ¢åˆ°ç¾å›½ï¼Œç³»ç»Ÿè¯­è¨€åˆ‡æ¢è‹±æ–‡
- ç¾åŒºè´¦å·ä¸‹è½½chatgpt app
- é€šè¿‡æ”¯ä»˜å®å°ç¨‹åºç»™ç¾åŒºè´¦å·å……å€¼ç¤¼å“å¡
- chatgpt appç™»å½•è´¦å·ï¼Œé€šè¿‡ç¾åŒºè´¦å·å……å€¼gpt4

å…¶ä»–ï¼šåˆ°è¿™ä¸ª[é“¾æ¥](https://platform.openai.com/api-keys)ä¸­ç™»å½•è·å–api key,ç„¶åéƒ¨ç½²åˆ°å…¶ä»–åœ°æ–¹(ä½†ç”±äºåé¢å¥½åƒè¿˜è¦å……å€¼ï¼Œæ²¡å†ç»§ç»­éƒ¨ç½²)

## Ollama

> Ref: <https://github.com/ollama/ollama>

### Deploy ollama with Chinese mirror

> Ref: <https://tianhao.tech/default/ollama-installation-guide-china.html>

```bash
curl -fsSL https://ollama.com/install.sh -o ollama_install.sh
```

Replate the download URL in `ollama_install.sh`:

```bash
#!/bin/bash

FILE="ollama_install.sh"

sed -i 's|https://ollama.com/download/ollama-linux-${ARCH}${VER_PARAM}|https://github.moeyy.xyz/https://github.com/ollama/ollama/releases/download/v0.3.4/ollama-linux-amd64|g' $FILE
sed -i 's|https://ollama.com/download/ollama-linux-amd64-rocm.tgz${VER_PARAM}|https://github.moeyy.xyz/https://github.com/ollama/ollama/releases/download/v0.3.4/ollama-linux-amd64-rocm.tgz|g' $FILE
```

```bash
http_proxy=127.0.0.1:7890 https_proxy=127.0.0.1:7890 ollama run deepseek-r1:32b
```

### Deploy ollama in autodl machine

> Ref: <https://blog.csdn.net/tirestay/article/details/139773544>

```bash
source /etc/network_turbo
sudo apt update
sudo apt install systemd systemctl lshw
curl -fsSL https://ollama.com/install.sh | sh
# systemctl start ollama.service
# kill -9 [ollama process number]
http_proxy=127.0.0.1:7890 https_proxy=127.0.0.1:7890 OLLAMA_MODELS=/root/autodl-tmp/ollama ollama serve
http_proxy=127.0.0.1:7890 https_proxy=127.0.0.1:7890 OLLAMA_MODELS=/root/autodl-tmp/ollama ollama run deepseek-r1:70b
```

## GPT Academic (in autodl machine)

> Ref: <https://github.com/binary-husky/gpt_academic>

```bash
source /etc/network_turbo
http_proxy=127.0.0.1:7890 https_proxy=127.0.0.1:7890 git clone --depth=1 https://github.com/binary-husky/gpt_academic.git
cd gpt_academic
cp config.py config_private.py
vim config_private.py
conda init
source ~/.bashrc
conda create -n gptac_venv python=3.11
conda activate gptac_venv
python -m pip install -r requirements.txt
python main.py
```

## Lobe Chat

> Ref: <https://github.com/lobehub/lobe-chat>

### Re-Deploy Lobe Chat with docker compose

æ³¨æ„ï¼šé‡æ–°å®‰è£…æ—¶ï¼Œåªéœ€è¦åˆ æ‰lobe-chatå®¹å™¨ï¼Œç„¶åæ‹‰å–æœ€æ–°lobe-chatå®¹å™¨ï¼Œæ‰§è¡Œ`docker compose up -d`ã€‚ä¸è¦åˆ é™¤`docker-compose.yml`å’Œ`init_data.json`ï¼Œä¹Ÿä¸è¦åˆ é™¤`data`å’Œ`s3_data`ã€‚å¦åˆ™ä¹‹å‰çš„æ•°æ®å°±ä¸¢å¤±äº†ã€‚ä»£ç æ˜¯å¦æ‹‰å–æœ€æ–°çš„æ— æ‰€è°“ã€‚å› ä¸ºä½¿ç”¨çš„å¹¶ä¸æ˜¯æœ€æ–°ä»£ç æä¾›çš„docker composeæ–‡ä»¶ï¼Œè€Œæ˜¯æœ¬åœ°çš„ã€‚

```bash
docker rm -f lobe-chat
docker pull lobehub/lobe-chat-database:latest
docker compose up -d  # Or: docker-compose up -d
```

### First deploy Lobe Chat with docker compose

```text
Based on: https://github.com/lobehub/lobe-chat
Branch: main
Commit: efb9311e6f3c
```

```bash
pwd
# /Users/yusongli/Documents

git clone git@github.com:ysl2/lobe-chat.git
cd lobe-chat/docker-compose/local
bash setup.sh
# Select the second option: Port mode

docker compose up -d
```

### Embedding model configuration

æŠŠè¿™é‡Œæ¢æˆsiliconflowçš„keyå’Œproxyï¼Œç”¨äºembeddingæ¨¡å‹

![](assets/AI-workflow-software/2025-07-06-12-05-53.png)

å…³é”®çš„`.env`é…ç½®å¦‚ä¸‹:

```bash
# DEFAULT_FILES_CONFIG=embedding_model=siliconcloud/Pro/BAAI/bge-m3
DEFAULT_FILES_CONFIG=embedding_model=siliconcloud/Qwen/Qwen3-Embedding-0.6B
```

PS: 413æ˜¯å› ä¸ºè¯¥æ¨¡å‹ä¸æ”¯æŒ1024ç»´ï¼Œæ¢äº†BAAIåæ­£å¸¸ã€‚

> Ref: åˆšåˆšå»çœ‹äº†ä»£ç å®ç°ï¼Œä»£ç å®ç°æ˜¯é€šè¿‡ä»`$DEFAULT_FILES_CONFIG` æˆªå– PROVIDER æ¥ç»„åˆæå–envä¸­å­˜åœ¨çš„`${PROVIDER}_API_KEY`,`${PROVIDER}_PROXY_URL` æ¥æ„å»ºå¯¹åº”çš„è¯·æ±‚çš„ï¼Œå¦‚æœæ²¡æœ‰`${PROVIDER}_API_KEY`ä¼šä½¿ç”¨`$OPENAI_API_KEY`æ¥å…œåº•ï¼Œå…œåº•è¿™ä¸ªè¡Œä¸ºæœ¬èº«éœ€è¦æ–Ÿé…Œï¼Œå› ä¸ºå¯èƒ½å¯¼è‡´æ²¡æœ‰æ­£ç¡®é…ç½®çš„ç¯å¢ƒæŠŠopenaiçš„keyæš´éœ²åˆ°ä¸åŒ¹é…çš„providerï¼Œä½†æ˜¯ä»£ç é€»è¾‘æœ¬èº«æ²¡æœ‰é—®é¢˜ã€‚
> æœ€åæˆ‘ä½¿ç”¨ç›¸åŒç¯å¢ƒä¸‹é‡æ–°éƒ¨ç½²äº†ä¸€æ¬¡ï¼Œæ²¡æœ‰å†æ¬¡å¤ç°è¿™ä¸ªé—®é¢˜ã€‚
