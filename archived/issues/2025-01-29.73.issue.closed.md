# [\#73 Issue](https://github.com/ysl2/.dotfiles/issues/73) `closed`: LLM相关

#### <img src="https://avatars.githubusercontent.com/u/39717545?u=3a56d7b47e1688f70c83e440ba0835f8d24c43e3&v=4" width="50">[ysl2](https://github.com/ysl2) opened issue at [2025-01-29 03:52](https://github.com/ysl2/.dotfiles/issues/73):

## 在线使用

> Ref: https://www.v2ex.com/t/1108382

中转API key提供方：
- https://luee.net/key ，速度快，但是感觉不是o3-mini
- https://siliconflow.cn ，可用deepseek。https://deepinfra.com


All in one：https://www.closechat.org/ ，速度快，并且是o3-mini，但是太贵
某宝还买了中转api，速度慢，只能o1-mini。直连key效果还行

前端：chatbox，桌面端/移动端都可用

## 本地部署

显卡：apple m3 silicon

后端：ollama

```
P ollama run deepseek-r1:32b
```

前端：gpt_academic

```
cp config.py config_private.py
python main.py
```




#### <img src="https://avatars.githubusercontent.com/u/39717545?u=3a56d7b47e1688f70c83e440ba0835f8d24c43e3&v=4" width="50">[ysl2](https://github.com/ysl2) commented at [2025-01-31 04:46](https://github.com/ysl2/.dotfiles/issues/73#issuecomment-2626290996):

## Ollama安装

### 国内下载参考

> Ref: https://tianhao.tech/default/ollama-installation-guide-china.html

```bash
curl -fsSL https://ollama.com/install.sh -o ollama_install.sh
```

替换下载链接：

```bash
#!/bin/bash

# 文件路径
FILE="ollama_install.sh"

# 修改 URL
sed -i 's|https://ollama.com/download/ollama-linux-${ARCH}${VER_PARAM}|https://github.moeyy.xyz/https://github.com/ollama/ollama/releases/download/v0.3.4/ollama-linux-amd64|g' $FILE
sed -i 's|https://ollama.com/download/ollama-linux-amd64-rocm.tgz${VER_PARAM}|https://github.moeyy.xyz/https://github.com/ollama/ollama/releases/download/v0.3.4/ollama-linux-amd64-rocm.tgz|g' $FILE
```

### autodl下载参考

> Ref: https://blog.csdn.net/tirestay/article/details/139773544

```bash
source /etc/network_turbo
sudo apt update
sudo apt install systemd systemctl lshw
curl -fsSL https://ollama.com/install.sh | sh
# systemctl start ollama.service
# kill -9 [ollama process number]
http_proxy=127.0.0.1:7890 https_proxy=127.0.0.1:7890 OLLAMA_MODELS=/root/autodl-tmp/ollama ollama serve
http_proxy=127.0.0.1:7890 https_proxy=127.0.0.1:7890 OLLAMA_MODELS=/root/autodl-tmp/ollama ollama run deepseek-r1:70b
```

#### <img src="https://avatars.githubusercontent.com/u/39717545?u=3a56d7b47e1688f70c83e440ba0835f8d24c43e3&v=4" width="50">[ysl2](https://github.com/ysl2) commented at [2025-01-31 05:38](https://github.com/ysl2/.dotfiles/issues/73#issuecomment-2626348287):

## GPT Academic安装

```bash
source /etc/network_turbo
http_proxy=127.0.0.1:7890 https_proxy=127.0.0.1:7890 git clone --depth=1 https://github.com/binary-husky/gpt_academic.git
cd gpt_academic
cp config.py config_private.py
vim config_private.py
conda init
source ~/.bashrc
conda create -n gptac_venv python=3.11
conda activate gptac_venv
python -m pip install -r requirements.txt
python main.py
```

#### <img src="https://avatars.githubusercontent.com/u/39717545?u=3a56d7b47e1688f70c83e440ba0835f8d24c43e3&v=4" width="50">[ysl2](https://github.com/ysl2) commented at [2025-02-16 02:52](https://github.com/ysl2/.dotfiles/issues/73#issuecomment-2661205562):

## LLMs

chatbot arena

## Webs

chatgpt next web
lobe chat 一键本地部署（选择第二个模式，port mode）：https://github.com/lobehub/lobe-chat/blob/main/docker-compose/setup.sh

## Apps

chatbox
cherry studio
anythingllm

## RAG

RAGFlow
dify (non-free)

## Others

deepclaude


-------------------------------------------------------------------------------



[Export of Github issue for [ysl2/.dotfiles](https://github.com/ysl2/.dotfiles). Generated on 2025.06.30 at 17:53:18.]
